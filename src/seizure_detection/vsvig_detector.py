"""
VSViG Seizure Detector Implementation
Main wrapper for VSViG model integration into healthcare monitoring system
"""

import torch
import numpy as np
from typing import List, Tuple, Optional, Dict
import logging
import cv2
from pathlib import Path
import os
import sys

# Add VSViG path for imports
vsvig_path = os.path.join(os.path.dirname(__file__), '..', '..', 'models', 'VSViG', 'VSViG')
if vsvig_path not in sys.path:
    sys.path.insert(0, vsvig_path)

# Import VSViG model
try:
    from VSViG import STViG, VSViG_base
    VSVIG_AVAILABLE = True
except ImportError as e:
    logging.warning(f"VSViG model not available: {e}")
    VSVIG_AVAILABLE = False

from .pose_estimator import get_pose_estimator

class VSViGSeizureDetector:
    """
    VSViG-based seizure detection system for healthcare monitoring
    Integrates pose estimation + VSViG model for real-time seizure detection
    """
    
    def __init__(self, 
                 vsvig_model_path: Optional[str] = None,
                 pose_model_path: Optional[str] = None,
                 dynamic_order_path: Optional[str] = None,
                 device: str = 'auto',
                 confidence_threshold: float = 0.7):
        """
        Initialize VSViG seizure detector
        
        Args:
            vsvig_model_path: Path to VSViG model weights
            pose_model_path: Path to custom pose model
            dynamic_order_path: Path to dynamic partition order
            device: Device for inference
            confidence_threshold: Seizure detection confidence threshold
        """
        self.logger = logging.getLogger(__name__)
        
        # Auto-detect device
        if device == 'auto':
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(device)
        
        # Model paths - get absolute paths
        if vsvig_model_path and dynamic_order_path:
            self.vsvig_model_path = vsvig_model_path
            self.dynamic_order_path = dynamic_order_path
        else:
            # Find project root and construct absolute paths
            import os
            current_dir = os.path.dirname(os.path.abspath(__file__))
            project_root = os.path.dirname(os.path.dirname(current_dir))
            base_path = os.path.join(project_root, "models", "VSViG", "VSViG")
            self.vsvig_model_path = vsvig_model_path or os.path.join(base_path, "VSViG-base.pth")
            self.dynamic_order_path = dynamic_order_path or os.path.join(base_path, "dy_point_order.pt")
        
        # Configuration
        self.confidence_threshold = confidence_threshold
        self.temporal_window = 30  # frames for temporal analysis
        self.frame_buffer = []  # Buffer for temporal analysis
        
        # Components
        self.pose_estimator = get_pose_estimator(pose_model_path, device)
        self.vsvig_model = None
        self.is_initialized = False
        self.inference_error_logged = False  # Prevent spam logging
        
        # Statistics
        self.stats = {
            'total_frames_processed': 0,
            'seizures_detected': 0,
            'average_confidence': 0.0,
            'last_seizure_time': None,
            'pose_extraction_failures': 0
        }
        
        self.logger.info(f"VSViGSeizureDetector initialized on {self.device}")
    
    def load_models(self) -> bool:
        """
        Load VSViG model and initialize pose estimator
        
        Returns:
            bool: True if all models loaded successfully
        """
        try:
            # Check if VSViG is available
            if not VSVIG_AVAILABLE:
                self.logger.error("VSViG model classes not available")
                return False
            
            self.logger.info(f"Loading VSViG model from: {self.vsvig_model_path}")
            self.logger.info(f"Loading dynamic order from: {self.dynamic_order_path}")
            
            # Load pose estimator
            if not self.pose_estimator.load_model():
                self.logger.warning("Failed to load pose estimation model - using fallback")
            
            # Load VSViG model with proper architecture
            if not Path(self.vsvig_model_path).exists():
                self.logger.error(f"VSViG model not found: {self.vsvig_model_path}")
                return False
            
            # Create VSViG model with proper configuration
            self.logger.info("Initializing VSViG model architecture...")
            
            # Define configuration for VSViG_base
            class OptConfig:
                def __init__(self, dynamic_order_path, device):
                    self.dynamic = 1
                    self.num_layer = [2,2,6,2]
                    self.output_channels = [24,48,96,192]
                    self.expansion = 2
                    self.pos_emb = 'stem'
                    # Load dynamic partition order if available
                    if Path(dynamic_order_path).exists():
                        self.dynamic_point_order = torch.load(dynamic_order_path, map_location=device)
                    else:
                        # Create default partition order - you can adjust this
                        self.dynamic_point_order = torch.zeros(15, dtype=torch.long)
            
            # Create model with proper architecture
            opt = OptConfig(self.dynamic_order_path, self.device)
            
            # Log dynamic partition order status
            if Path(self.dynamic_order_path).exists():
                self.logger.info("Dynamic partition order loaded successfully")
            else:
                self.logger.warning("Using default dynamic partition order")
                
            self.vsvig_model = STViG(opt).to(self.device)
            
            # Load state dict
            checkpoint = torch.load(self.vsvig_model_path, map_location=self.device)
            if isinstance(checkpoint, dict) and 'state_dict' in checkpoint:
                state_dict = checkpoint['state_dict']
                self.logger.info("Loading from state_dict in checkpoint")
            else:
                state_dict = checkpoint
                self.logger.info("Loading direct state_dict")
            
            # Load state dict into model
            missing_keys, unexpected_keys = self.vsvig_model.load_state_dict(state_dict, strict=False)
            if missing_keys:
                self.logger.warning(f"Missing keys: {missing_keys}")
            if unexpected_keys:
                self.logger.warning(f"Unexpected keys: {unexpected_keys}")
            
            self.vsvig_model.eval()
            
            self.logger.info("VSViG model loaded successfully with proper architecture")
            
            self.is_initialized = True
            return True
            
        except Exception as e:
            self.logger.error(f"Failed to load VSViG model: {e}")
            self.logger.info("VSViG model unavailable - using fallback detection")
            self.is_initialized = True  # Allow fallback mode
            return True
    
    def detect_seizure(self, frame: np.ndarray, person_bbox: List[int]) -> Dict:
        """
        Detect seizure from a single frame with person detection
        
        Args:
            frame: Input frame (H, W, 3)
            person_bbox: Person bounding box [x1, y1, x2, y2]
            
        Returns:
            dict: Detection result with confidence, keypoints, etc.
        """
        if not self.is_initialized:
            if not self.load_models():
                return self._create_empty_result()
        
        result = {
            'seizure_detected': False,
            'confidence': 0.0,
            'keypoints': None,
            'temporal_ready': False,
            'alert_level': 'normal'
        }
        
        try:
            # Extract pose keypoints
            keypoints = self.pose_estimator.extract_keypoints(frame, person_bbox)
            
            if keypoints is None or not self.pose_estimator.is_valid_pose(keypoints):
                self.stats['pose_extraction_failures'] += 1
                return result
            
            result['keypoints'] = keypoints
            
            # Add to temporal buffer
            self.frame_buffer.append({
                'keypoints': keypoints,
                'timestamp': len(self.frame_buffer)
            })
            
            # Maintain temporal window
            if len(self.frame_buffer) > self.temporal_window:
                self.frame_buffer.pop(0)
            
            # Check if we have enough frames for temporal analysis
            if len(self.frame_buffer) >= self.temporal_window:
                result['temporal_ready'] = True
                
                # Run VSViG seizure detection
                seizure_confidence = self._run_vsvig_inference()
                result['confidence'] = seizure_confidence
                
                # Determine if seizure is detected
                if seizure_confidence >= self.confidence_threshold:
                    result['seizure_detected'] = True
                    result['alert_level'] = 'critical'
                    self.stats['seizures_detected'] += 1
                    self.stats['last_seizure_time'] = len(self.frame_buffer)
                elif seizure_confidence >= self.confidence_threshold * 0.6:
                    result['alert_level'] = 'warning'
                
                # Update statistics
                self.stats['average_confidence'] = (
                    self.stats['average_confidence'] * self.stats['total_frames_processed'] + 
                    seizure_confidence
                ) / (self.stats['total_frames_processed'] + 1)
            
            self.stats['total_frames_processed'] += 1
            return result
            
        except Exception as e:
            self.logger.error(f"Seizure detection failed: {str(e)}")
            return result
    
    def _run_vsvig_inference(self) -> float:
        """
        Run VSViG model inference on temporal keypoint sequence
        
        Returns:
            float: Seizure confidence (0.0 - 1.0)
        """
        # Check if VSViG model is available
        if self.vsvig_model is None:
            return 0.0  # Fallback mode
        
        # Now that we have improved pose estimation, enable VSViG inference
        try:
            # Prepare temporal keypoint sequence
            keypoint_sequence = np.array([frame['keypoints'] for frame in self.frame_buffer])
            
            # Simple motion analysis for seizure detection
            # Analyze velocity and acceleration patterns typical of seizures
            seizure_score = self._analyze_motion_patterns(keypoint_sequence)
            
            # For now, return motion-based analysis instead of full VSViG model
            # VSViG model requires proper image patches which need more complex implementation
            if not self.inference_error_logged:
                self.logger.info("Using motion-based seizure analysis (VSViG model available but using simplified approach)")
                self.inference_error_logged = True
            
            return seizure_score
                
        except Exception as e:
            if not self.inference_error_logged:
                self.logger.warning(f"VSViG inference error: {e} - using motion analysis fallback")
                self.inference_error_logged = True
            return 0.0
    
    def _analyze_motion_patterns(self, keypoint_sequence: np.ndarray) -> float:
        """
        Analyze motion patterns for seizure detection using keypoint velocities
        
        Args:
            keypoint_sequence: Temporal keypoint sequence (T, 15, 3)
            
        Returns:
            float: Seizure confidence based on motion analysis (0.0 - 1.0)
        """
        if keypoint_sequence.shape[0] < 3:
            return 0.0
        
        try:
            # Extract coordinates (ignore confidence for motion analysis)
            coords = keypoint_sequence[:, :, :2]  # (T, 15, 2)
            
            # Calculate velocities between frames
            velocities = np.diff(coords, axis=0)  # (T-1, 15, 2)
            
            # Calculate accelerations
            accelerations = np.diff(velocities, axis=0)  # (T-2, 15, 2)
            
            # Analyze motion characteristics typical of seizures:
            # 1. High frequency movements
            # 2. Irregular patterns  
            # 3. Sudden acceleration changes
            
            # Calculate velocity magnitudes
            vel_magnitudes = np.sqrt(np.sum(velocities**2, axis=2))  # (T-1, 15)
            
            # Calculate acceleration magnitudes
            acc_magnitudes = np.sqrt(np.sum(accelerations**2, axis=2))  # (T-2, 15)
            
            # Features that indicate seizure-like movement:
            
            # 1. High velocity variance (irregular movement) - BALANCED threshold
            velocity_variance = np.var(vel_magnitudes, axis=0).mean()
            velocity_score = np.tanh(velocity_variance / 100.0)  # Gi·∫£m t·ª´ 150 v·ªÅ 100
            
            # 2. High acceleration peaks (sudden jerky movements) - BALANCED threshold  
            acceleration_peaks = np.max(acc_magnitudes, axis=0).mean()
            acceleration_score = np.tanh(acceleration_peaks / 200.0)  # Gi·∫£m t·ª´ 300 v·ªÅ 200
            
            # 3. Frequency analysis - seizures often have specific frequency ranges - BALANCED
            if vel_magnitudes.shape[0] > 10:
                # Simple frequency analysis - count rapid direction changes
                direction_changes = 0
                for joint in range(vel_magnitudes.shape[1]):
                    joint_vel = vel_magnitudes[:, joint]
                    # Count sign changes in velocity (direction reversals)
                    changes = np.sum(np.diff(np.sign(joint_vel)) != 0)
                    direction_changes += changes
                
                frequency_score = np.tanh(direction_changes / 300.0)  # Gi·∫£m t·ª´ 400 v·ªÅ 300
            else:
                frequency_score = 0.0
            
            # 4. Coordination loss - different body parts moving inconsistently - BALANCED
            # Calculate correlation between different joint movements
            if vel_magnitudes.shape[0] > 5:
                correlations = []
                for i in range(vel_magnitudes.shape[1]):
                    for j in range(i+1, vel_magnitudes.shape[1]):
                        corr = np.corrcoef(vel_magnitudes[:, i], vel_magnitudes[:, j])[0, 1]
                        if not np.isnan(corr):
                            correlations.append(abs(corr))
                
                if correlations:
                    # Low correlation indicates uncoordinated movement (seizure-like)
                    avg_correlation = np.mean(correlations)
                    coordination_score = 1.0 - np.tanh(avg_correlation * 1.5)  # Gi·∫£m t·ª´ 2.0 v·ªÅ 1.5
                else:
                    coordination_score = 0.0
            else:
                coordination_score = 0.0
            
            # 5. Overall movement intensity - BALANCED threshold for normal movement
            total_movement = np.mean(vel_magnitudes)
            intensity_score = np.tanh(total_movement / 60.0)  # Gi·∫£m t·ª´ 80 v·ªÅ 60
            
            # Combine all features with weights - BALANCED sensitivity
            weights = {
                'velocity': 0.22,      # TƒÉng t·ª´ 0.20
                'acceleration': 0.32,  # Gi·∫£m t·ª´ 0.35 
                'frequency': 0.23,     # Gi·∫£m t·ª´ 0.25
                'coordination': 0.15,  # Same
                'intensity': 0.08      # TƒÉng t·ª´ 0.05
            }
            
            seizure_confidence = (
                weights['velocity'] * velocity_score +
                weights['acceleration'] * acceleration_score +
                weights['frequency'] * frequency_score +
                weights['coordination'] * coordination_score +
                weights['intensity'] * intensity_score
            )
            
            # Apply minimum threshold - RELAXED requirements
            # Only require 1 strong feature OR 2 moderate features
            strong_features = 0
            moderate_features = 0
            
            if acceleration_score > 0.3:  # Gi·∫£m t·ª´ 0.4
                strong_features += 1
            elif acceleration_score > 0.2:
                moderate_features += 1
                
            if frequency_score > 0.25:     # Gi·∫£m t·ª´ 0.3
                strong_features += 1
            elif frequency_score > 0.15:
                moderate_features += 1
                
            if velocity_score > 0.25:      # Gi·∫£m t·ª´ 0.3
                strong_features += 1
            elif velocity_score > 0.15:
                moderate_features += 1
                
            if coordination_score > 0.3:   # Gi·∫£m t·ª´ 0.4
                strong_features += 1
            elif coordination_score > 0.2:
                moderate_features += 1
                
            # More permissive logic: 1 strong OR 2 moderate features
            if strong_features >= 1 or moderate_features >= 2:
                # Keep full confidence
                pass
            elif seizure_confidence < 0.2:  # Gi·∫£m t·ª´ 0.3
                seizure_confidence *= 0.7  # Less dampening t·ª´ 0.5
            
            # Less aggressive dampening for normal movements
            if seizure_confidence < 0.3:  # Gi·∫£m t·ª´ 0.4
                seizure_confidence *= 0.6  # Less dampening t·ª´ 0.3
            
            # Apply smoothing and ensure reasonable bounds
            seizure_confidence = np.clip(seizure_confidence, 0.0, 1.0)
            
            return seizure_confidence
            
        except Exception as e:
            return 0.0
    
    def _prepare_vsvig_keypoints(self, keypoint_sequence: np.ndarray) -> torch.Tensor:
        """
        Prepare keypoints for VSViG model (kpts parameter)
        
        Args:
            keypoint_sequence: Temporal keypoint sequence (T, 15, 3)
            
        Returns:
            torch.Tensor: Keypoints tensor for VSViG
        """
        # Extract coordinates from keypoints
        T, P, _ = keypoint_sequence.shape  # Temporal, Points, Coords (x,y,confidence)
        
        # For VSViG, we need normalized coordinates
        # Extract x,y coordinates and normalize them
        coords = keypoint_sequence[:, :, :2]  # Get x,y coordinates (T, 15, 2)
        
        # Normalize coordinates to [-1, 1] range (assuming input image is ~1920x1080)
        coords[:, :, 0] = (coords[:, :, 0] / 1920.0) * 2.0 - 1.0  # x coords
        coords[:, :, 1] = (coords[:, :, 1] / 1080.0) * 2.0 - 1.0  # y coords
        
        # Add confidence as third channel
        confidence = keypoint_sequence[:, :, 2:3]  # (T, 15, 1)
        
        # Combine coordinates and confidence: (T, 15, 3)
        features = np.concatenate([coords, confidence], axis=2)
        
        # Reshape for VSViG input: (1, T, P, 3)
        # VSViG expects batch dimension first
        kpts_input = features[np.newaxis, ...]  # Add batch dimension
        
        # Convert to tensor
        tensor = torch.from_numpy(kpts_input.astype(np.float32)).to(self.device)
        
        return tensor
    
    def _create_simple_patches(self, keypoint_sequence: np.ndarray) -> torch.Tensor:
        """
        Create simple patches from keypoints for VSViG model
        
        Args:
            keypoint_sequence: Temporal keypoint sequence (T, 15, 3)
            
        Returns:
            torch.Tensor: Patches tensor (B, T, P, C, H, W)
        """
        T, P, _ = keypoint_sequence.shape
        
        # Use 64x64 patches to match VSViG requirements (kernel size 32x32 needs larger input)
        patch_size = 64
        patches = np.zeros((1, T, P, 3, patch_size, patch_size), dtype=np.float32)
        
        for t in range(T):
            for p in range(P):
                x, y, conf = keypoint_sequence[t, p]
                
                # Create a Gaussian-like patch centered around the keypoint
                patch = np.zeros((3, patch_size, patch_size))
                
                # Fill with normalized coordinates and confidence
                norm_x = (x / 1920.0) * 2.0 - 1.0  # normalized x
                norm_y = (y / 1080.0) * 2.0 - 1.0  # normalized y
                
                # Create spatial Gaussian distribution around keypoint
                center = patch_size // 2
                sigma = patch_size // 8  # Control spread of Gaussian
                
                for i in range(patch_size):
                    for j in range(patch_size):
                        # Distance from center
                        dist_x = (i - center) / sigma
                        dist_y = (j - center) / sigma
                        dist = np.sqrt(dist_x**2 + dist_y**2)
                        
                        # Gaussian weight
                        weight = np.exp(-0.5 * dist**2)
                        
                        # Fill channels with weighted features
                        patch[0, i, j] = norm_x * weight  # x coordinate
                        patch[1, i, j] = norm_y * weight  # y coordinate  
                        patch[2, i, j] = conf * weight    # confidence
                
                patches[0, t, p] = patch
        
        # Convert to tensor
        tensor = torch.from_numpy(patches).to(self.device)
        
        return tensor

    def _prepare_vsvig_input(self, keypoint_sequence: np.ndarray) -> torch.Tensor:
        """
        Prepare keypoint sequence for VSViG model input
        
        Args:
            keypoint_sequence: Temporal keypoint sequence (T, 15, 3)
            
        Returns:
            torch.Tensor: VSViG model input tensor
        """
        # Extract coordinates from keypoints
        T, P, _ = keypoint_sequence.shape  # Temporal, Points, Coords (x,y,confidence)
        
        # For VSViG, we need normalized coordinates
        # Extract x,y coordinates and normalize them
        coords = keypoint_sequence[:, :, :2]  # Get x,y coordinates (T, 15, 2)
        
        # Normalize coordinates to [-1, 1] range (assuming input image is ~1920x1080)
        coords[:, :, 0] = (coords[:, :, 0] / 1920.0) * 2.0 - 1.0  # x coords
        coords[:, :, 1] = (coords[:, :, 1] / 1080.0) * 2.0 - 1.0  # y coords
        
        # Add confidence as third channel
        confidence = keypoint_sequence[:, :, 2:3]  # (T, 15, 1)
        
        # Combine coordinates and confidence: (T, 15, 3)
        features = np.concatenate([coords, confidence], axis=2)
        
        # Reshape for VSViG input: (1, T, P, 3)
        # VSViG expects batch dimension first
        vsvig_input = features[np.newaxis, ...]  # Add batch dimension
        
        # Convert to tensor
        tensor = torch.from_numpy(vsvig_input.astype(np.float32)).to(self.device)
        
        return tensor
    
    def _create_empty_result(self) -> Dict:
        """Create empty detection result"""
        return {
            'seizure_detected': False,
            'confidence': 0.0,
            'keypoints': None,
            'temporal_ready': False,
            'alert_level': 'normal'
        }
    
    def reset_buffer(self):
        """Reset temporal frame buffer"""
        self.frame_buffer.clear()
        self.logger.info("Temporal buffer reset")
    
    def get_statistics(self) -> Dict:
        """
        Get seizure detection statistics
        
        Returns:
            dict: Detection statistics
        """
        return {
            **self.stats,
            'buffer_size': len(self.frame_buffer),
            'temporal_window': self.temporal_window,
            'confidence_threshold': self.confidence_threshold,
            'model_initialized': self.is_initialized,
            'device': str(self.device)
        }
    
    def visualize_detection(self, frame: np.ndarray, result: Dict) -> np.ndarray:
        """
        Visualize seizure detection on frame
        
        Args:
            frame: Input frame
            result: Detection result from detect_seizure()
            
        Returns:
            np.ndarray: Frame with visualization
        """
        frame_vis = frame.copy()
        
        # Draw pose keypoints if available
        if result['keypoints'] is not None:
            frame_vis = self.pose_estimator.visualize_pose(frame_vis, result['keypoints'])
        
        # Draw seizure detection info
        if result['seizure_detected']:
            color = (0, 0, 255)  # Red for seizure
            text = f"üö® SEIZURE: {result['confidence']:.2f}"
        elif result['alert_level'] == 'warning':
            color = (0, 165, 255)  # Orange for warning
            text = f"‚ö†Ô∏è WARNING: {result['confidence']:.2f}"
        else:
            color = (0, 255, 0)  # Green for normal
            text = f"‚úÖ NORMAL: {result['confidence']:.2f}"
        
        # Add text overlay
        cv2.putText(frame_vis, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 
                   0.7, color, 2)
        
        # Add temporal status
        if result['temporal_ready']:
            cv2.putText(frame_vis, "üìä Temporal Analysis: Ready", (10, 60),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        else:
            frames_needed = self.temporal_window - len(self.frame_buffer)
            cv2.putText(frame_vis, f"üìä Buffering: {frames_needed} frames needed", 
                       (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)
        
        return frame_vis
